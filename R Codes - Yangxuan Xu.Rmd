---
title: "R codes"
author: "Yangxuan Xu"
output: pdf_document
---
---
title: "Multivariate Regression Model for Explaining House Price"
author: "Yangxuan Xu"
output: pdf_document
---
# Data Import & Variables Checking
```{r}
library(readxl)
dat <- read_excel("Data/Real estate valuation data set.xlsx")
dat=dat[-1]
colnames(dat)=c("x1","x2","x3","x4","x5","x6","y")
anyNA(dat) # check if any missing value
summary(dat,digits=6)
```
# Visualization
```{r}
par(mfrow=c(2,3))
plot(dat$y~dat$x1,xlab='x1',ylab='y')
plot(dat$y~dat$x2,xlab='x2',ylab='y')
plot(dat$y~dat$x3,xlab='x3',ylab='y')
plot(dat$y~dat$x4,xlab='x4',ylab='y')
plot(dat$y~dat$x5,xlab='x5',ylab='y')
plot(dat$y~dat$x6,xlab='x6',ylab='y')
# patterns between inputs and response
par(mfrow=c(1,1))
library(ggplot2)
ggplot()+geom_point(data=dat,aes(x=x6,y=x5,col=y)) #geographical plots
```

# Variable Selection/Model Construction
```{r}
lm0=lm(y~(x1+x2+x3+x4+x5+x6)^2,data=dat)
summary(lm0)

# do step-wise variable selection
step(lm0,direction="both",trace=FALSE)

lm1=lm(y~x1+x2+x3+x4+x5+x6+x2:x6+x3:x4+x3:x5+x4:x5,data=dat)
summary(lm1)

# drop x6 and x2:x6
lm2=lm(y~x1+x2+x3+x4+x5+x3:x4+x3:x5+x4:x5,data=dat)
summary(lm2)

library(faraway)
# check multicollinearity
vif(lm2)
# there exists multicollinearity
# drop x3:x5 and x4:x5
lm_1=lm(log(y)~x1+x2+x3+x4+x5+x3:x4,data=dat)
summary(lm_1) # predictors are significant
vif(lm_1) # no multicollinearity
```

```{r}
plot(dat$y,ylab='y')
abline(h=70,col='red',lty=2)
sum(dat$y>70)
center_x=mean(dat[which(dat$y>70),]$x6)
center_y=mean(dat[which(dat$y>70),]$x5)
c(center_x,center_y)
dat$r=sqrt((dat$x6-center_x)^2+(dat$x5-center_y)^2)
dat$theta=atan((dat$x6-center_x)/(dat$x5-center_y))


lm4=lm(log(y)~(x1+x2+x3+x4+r+theta)^2,data=dat)
summary(lm4)

# do step-wise variable selection
step(lm4,direction="both",trace=FALSE)

lm5=lm(log(y)~x1+x2+x3+x4+r+theta+x1:x4+x2:x3+x2:r+x3:x4+x3:r+x3:theta+x4:r,data=dat)
summary(lm5)

# drop insignificant predictors
lm6=lm(log(y)~x1+x2+x3+x4+r+theta+x2:x3+x2:r+x3:r+x3:theta,data=dat)
summary(lm6)

# try to drop some interactions
lm7=lm(log(y)~x1+x2+x3+x4+x2:x3+x2:r+x3:r+theta+x3:theta,data=dat)
summary(lm7)
library(lmtest)
lrtest(lm7,lm6) # compare lm6 and lm7 --- lm6 is better
vif(lm6) # multicollinearity exists

lm8=lm(log(y)~x1+x2+log(x3)+x4+r,data=dat)
summary(lm8)
vif(lm8) # no multicollinearity

lm_2=lm8
```
# Model Diagnostics
#Model(1)
```{r}
library(zoo)
par(mfrow=c(1,2))
plot(fitted(lm_1),resid(lm_1),xlab='Fitted',ylab='Residual',
     main='Fitted vs Residuals',
     col = "grey", 
     pch = 20)
abline(h=0, col = "orange", lwd = 3)
qqnorm(resid(lm_1),col="grey",pch=20)
qqline(resid(lm_1),col="green",lwd=2)
library(lmtest)
bptest(lm_1) # equal variance holds
shapiro.test(resid(lm_1)) # normality is violated
dwtest(lm_1) # no autocorrelation


# try box-cox transformation
library(MASS)
boxcox(lm_1)
boxcox(lm_1, lambda = seq(0, 0.5, by = 0.05))
# Let's transform Y using lambda = 0.42
lambda = 0.42
dat_lm_transf=lm(((log(y)^(lambda)-1)/(lambda))~x1+x2+x3+x4+x5+x3:x4,data=dat)
summary(dat_lm_transf)
par(mfrow=c(1,2))
plot(fitted(dat_lm_transf),resid(dat_lm_transf),
     xlab='Fitted',ylab='Residual',
     main='Fitted vs Residuals',
     col = "grey", 
     pch = 20)
abline(h=0, col = "orange", lwd = 3)
qqnorm(resid(dat_lm_transf),col="grey",pch=20)
qqline(resid(dat_lm_transf),col="green",lwd=2)

bptest(dat_lm_transf)
shapiro.test(resid(dat_lm_transf))
dwtest(dat_lm_transf)
# The Box-Cox transformation doesn't help to correct normality
```
#Model(2)
```{r}
par(mfrow=c(1,2))
plot(fitted(lm_2),resid(lm_2),xlab='Fitted',ylab='Residual',
     main='Fitted vs Residuals',
     col = "grey", 
     pch = 20)
abline(h=0, col = "orange", lwd = 3)
qqnorm(resid(lm_2),col="grey",pch=20)
qqline(resid(lm_2),col="green",lwd=2)
library(lmtest)
bptest(lm_2) # equal variance holds
shapiro.test(resid(lm_2)) # normality is violated
dwtest(lm_2) # no autocorrelation

# try box-cox transformation
library(MASS)
boxcox(lm_2)
boxcox(lm_2, lambda = seq(0, 0.5, by = 0.05))
# Let's transform Y using lambda = 0.42
lambda = 0.42
dat_lm_transf2=lm(((log(y)^(lambda)-1)/(lambda))~x1+x2+log(x3)+x4+r,data=dat)
summary(dat_lm_transf2)
par(mfrow=c(1,2))
plot(fitted(dat_lm_transf2),resid(dat_lm_transf2),
     xlab='Fitted',ylab='Residual',
     main='Fitted vs Residuals',
     col = "grey", 
     pch = 20)
abline(h=0, col = "orange", lwd = 3)
qqnorm(resid(dat_lm_transf2),col="grey",pch=20)
qqline(resid(dat_lm_transf2),col="green",lwd=2)

library(lmtest)
bptest(dat_lm_transf2)
shapiro.test(resid(dat_lm_transf2))
dwtest(dat_lm_transf2)
# The Box-Cox transformation doesn't help
```

# Outliers,Influential Points
```{r}
sum(abs(rstandard(lm_1)) > 2)
sum(cooks.distance(lm_1) > 4 / length(cooks.distance(lm_1)))

sum(abs(rstandard(lm_2)) > 2)
sum(cooks.distance(lm_2) > 4 / length(cooks.distance(lm_2)))
```


# Evaluation
```{r}
library(lmtest)
lrtest(lm_2,lm_1)
# p-value=0.1518 is large, we have no evidence to say lm_1 is better
AIC(lm_1,lm_2)
BIC(lm_1,lm_2)
summary(lm_1)$adj.r.square
summary(lm_2)$adj.r.square

coef(lm_2)
exp(coef(lm_2))
```
# Furthur discussion
```{r}
# try to use different thresholds for choosing origin of model(2)
center_x=mean(dat[which(dat$y>0),]$x6)
center_y=mean(dat[which(dat$y>0),]$x5)
dat$r=sqrt((dat$x6-center_x)^2+(dat$x5-center_y)^2)
dat$theta=atan((dat$x6-center_x)/(dat$x5-center_y))
lm_2_1=lm(log(y)~x1+x2+log(x3)+x4+r,data=dat)
lrtest(lm_2_1,lm_1)
# lm_1 is better, p-value is very small

center_x=mean(dat[which(dat$y>30),]$x6)
center_y=mean(dat[which(dat$y>30),]$x5)
dat$r=sqrt((dat$x6-center_x)^2+(dat$x5-center_y)^2)
dat$theta=atan((dat$x6-center_x)/(dat$x5-center_y))
lm_2_2=lm(log(y)~x1+x2+log(x3)+x4+r,data=dat)
lrtest(lm_2_2,lm_1)
# lm_1 is better, p-value is larger but still small

center_x=mean(dat[which(dat$y>50),]$x6)
center_y=mean(dat[which(dat$y>50),]$x5)
dat$r=sqrt((dat$x6-center_x)^2+(dat$x5-center_y)^2)
dat$theta=atan((dat$x6-center_x)/(dat$x5-center_y))
lm_2_3=lm(log(y)~x1+x2+log(x3)+x4+r,data=dat)
lrtest(lm_2_3,lm_1)
# lm_1 is better, p-value is larger but still small

lrtest(lm_2,lm_1)
# p-value is large, lm_2 is better

center_x=mean(dat[which(dat$y>80),]$x6)
center_y=mean(dat[which(dat$y>80),]$x5)
dat$r=sqrt((dat$x6-center_x)^2+(dat$x5-center_y)^2)
dat$theta=atan((dat$x6-center_x)/(dat$x5-center_y))
lm_2_4=lm(log(y)~x1+x2+log(x3)+x4+r,data=dat)
lrtest(lm_2_4,lm_1)
# lm_1 is better, p-value is very small
```



